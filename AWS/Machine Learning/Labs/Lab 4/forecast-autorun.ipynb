{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Academy Machine Learning Foundations - Amazon Forecast Lab\n",
    "\n",
    "## STOP! - This file is automatically run on lab startup. Do not attempt to run the cells!\n",
    "\n",
    "This Jupyter notebook is part of the Amazon Forecast student lab. It was ran when the lab was created.\n",
    "\n",
    "If you just started the lab, load `forecast-lab.ipynb` and work through that notebook instead of this one.\n",
    "\n",
    "If you were directed to this file, read through the cells and explanations. Avoid running the cells.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook summary\n",
    "\n",
    "This notebook loads and preprocesses the online retail dataset. The data is uploaded to Amazon Simple Storage Service (Amazon S3), where it is used to create a forecast through Amazon Forecast. The notebook performs the following steps:\n",
    "\n",
    "- **Importing and functions** imports the packages used and creates helper functions.\n",
    "- **Importing data** downloads and loads the data into a pandas DataFrame.\n",
    "- **Data preprocessing** filters the data that is ready for training\n",
    "- **Generating training and testing DataFrames** downsamples the data to a daily frequency and splits the dataset into training and testing DataFrames.\n",
    "- **Uploading to Amazon S3** uploads the DataFrames to Amazon S3 as comma-separated values (CSV) files.\n",
    "- **Creating the Amazon Forecast dataset group** creates the project dataset group.\n",
    "- **Creating the datasets** creates the datasets in the dataset group and waits for the import to complete.\n",
    "- **Creating the predictor** trains the predictor by using the dataset group.\n",
    "- **Getting accuracy metrics** displays the metrics for the predictor.\n",
    "- **Creating the forecast** creates a test forecast.\n",
    "- **Optional cleanup** can perform a cleanup if it isn't completed in the `forecast-lab.ipynb` notebook.\n",
    "\n",
    "This notebook takes between 60–90 minutes to complete.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and functions\n",
    "\n",
    "The following code imports these packages:\n",
    "\n",
    "- *boto3* represents the AWS SDK for Python (Boto3), which is the Python library for AWS\n",
    "- *pandas* provides DataFrames for manipulating time series data\n",
    "- *matplotlib* provides plotting functions\n",
    "- *sagemaker* represents the API that's needed to work with Amazon SageMaker\n",
    "- *time*, *sys*, *os*, *io*, and *json* provide helper functions \n",
    "\n",
    "In addition, two helper functions are created:\n",
    "\n",
    "- `upload_s3_csv` uploads pandas DataFrames to Amazon S3 as CSV files. The header is removed, but *not* the index.\n",
    "- `StatusIndicator` provides a status function for long-running API calls to Amazon Forecast.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "bucket_name='c95096a2133910l4766086t1w419842340-forecastbucket-piqwf7cgzyor'\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time, sys, os, io, json\n",
    "import sagemaker\n",
    "!pip3 install pandas==1.5.3\n",
    "\n",
    "%store bucket_name\n",
    "\n",
    "s3_resource = boto3.Session().resource('s3')\n",
    "\n",
    "def upload_s3_csv(filename, folder, dataframe):\n",
    "    csv_buffer = io.StringIO()\n",
    "    dataframe.to_csv(filename, header=False, index=True)\n",
    "    dataframe.to_csv(csv_buffer, header=False, index=True )\n",
    "    s3_resource.Bucket(bucket_name).Object(os.path.join(prefix, folder, filename)).put(Body=csv_buffer.getvalue())\n",
    "\n",
    "class StatusIndicator:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.previous_status = None\n",
    "        self.need_newline = False\n",
    "        \n",
    "    def update( self, status ):\n",
    "        if self.previous_status != status:\n",
    "            if self.need_newline:\n",
    "                sys.stdout.write(\"\\n\")\n",
    "            sys.stdout.write( status + \" \")\n",
    "            self.need_newline = True\n",
    "            self.previous_status = status\n",
    "        else:\n",
    "            sys.stdout.write(\".\")\n",
    "            self.need_newline = True\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    def end(self):\n",
    "        if self.need_newline:\n",
    "            sys.stdout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data\n",
    "\n",
    "The following cell downloads the dataset, which is an Microsoft Excel file. This file is loaded into pandas as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "session = boto3.Session()\n",
    "forecast = session.client(service_name='forecast') \n",
    "forecast_query = session.client(service_name='forecastquery')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "The following cell completes the following preprocessing steps:\n",
    "\n",
    "- Removes instances with missing values\n",
    "- Sets the index to the InvoiceDate feature\n",
    "- Removes instances that aren't from the United Kingdom\n",
    "- Removes instances that don't use the target stock code (21232)\n",
    "- Keeps instances where the price is greater than 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail = pd.read_excel('online_retail_II.xlsx',engine='openpyxl')\n",
    "retail = retail.dropna()\n",
    "retail['InvoiceDate'] = pd.to_datetime(retail.InvoiceDate)\n",
    "retail = retail.set_index('InvoiceDate')\n",
    "\n",
    "country_filter = ['United Kingdom']\n",
    "retail = retail[retail['Country'].isin(country_filter)]\n",
    "\n",
    "#stockcodes = ['ADJUST', 'ADJUST2', 'POST', 'M']\n",
    "#stockcodes = [21232,22423]\n",
    "stockcodes = [21232]\n",
    "retail = retail[retail.StockCode.isin(stockcodes)]\n",
    "\n",
    "retail = retail[retail['Price']>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the training and testing DataFrames\n",
    "\n",
    "The following cell:\n",
    "\n",
    "- Splits the data into time series and related times series pandas DataFrames.\n",
    "- Downsamples the data from multiple sales entries per day into a single daily value. The **Quantity** column is summed, and the mean is used for the **Price** column.\n",
    "- Splits the DataFrames into a training set of data from January 2010–October 2010, and a testing set of data from November 2010–December 2010.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "retail_timeseries = retail[['StockCode','Quantity']]\n",
    "\n",
    "retail_timeseries = retail_timeseries.groupby('StockCode').resample('D').sum().reset_index().set_index(['InvoiceDate'])\n",
    "\n",
    "df_related_time_series = retail[['StockCode','Price']]\n",
    "df_related_time_series2 = df_related_time_series.groupby('StockCode').resample('D').mean().reset_index().set_index(['InvoiceDate'])\n",
    "df_related_time_series3 = df_related_time_series2.groupby('StockCode').pad()\n",
    "\n",
    "#df_related_time_series4 = df_related_time_series3.reset_index().set_index('InvoiceDate')\n",
    "\n",
    "# Select January to November for one DataFrame.\n",
    "jan_to_oct = retail_timeseries['2009-12':'2010-10']\n",
    "nov_to_dec = retail_timeseries['2010-11':'2010-12']\n",
    "jan_to_oct_related = df_related_time_series2['2009-12':'2010-10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_related_time_series2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading to Amazon S3\n",
    "\n",
    "The following cell uploads the DataFrames to Amazon S3 by using the helper function that was created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prefix='lab_4'\n",
    "train='../autorun/retail_ts_train.csv'\n",
    "train_related='../autorun/related_ts_train.csv'\n",
    "test='../autorun/retail_ts_test.csv'\n",
    "\n",
    "key=prefix + '/forecast/' + train\n",
    "# key='lab_4_forecast_t/forecast/retail_time_series_train.csv'\n",
    "related_key = prefix + '/forecast/' + train_related\n",
    "# related_key='lab_4_forecast_t/forecast/related.csv'\n",
    "\n",
    "upload_s3_csv(train, 'forecast', jan_to_oct)\n",
    "upload_s3_csv(train_related, 'forecast', jan_to_oct_related)\n",
    "upload_s3_csv(test, 'forecast', nov_to_dec)\n",
    "\n",
    "dataset_frequency = \"D\" \n",
    "timestamp_format = \"yyyy-MM-dd\"\n",
    "\n",
    "# project = prefix\n",
    "dataset_name = prefix+'_ds'\n",
    "related_dataset_name = prefix+'_rds'\n",
    "dataset_group_name = prefix +'_dsg'\n",
    "\n",
    "s3_data_path = \"s3://\"+bucket_name+\"/\"+key\n",
    "s3_related_data_path = \"s3://\"+bucket_name+\"/\"+related_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_to_oct_related.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%store prefix\n",
    "%store train\n",
    "%store test\n",
    "%store key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Amazon Forecast dataset group\n",
    "\n",
    "The following cell creates the dataset group for the forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_group_arn = None\n",
    "dsgs = forecast.list_dataset_groups()\n",
    "for dsg in dsgs['DatasetGroups']:\n",
    "    if dsg['DatasetGroupName']==dataset_group_name:\n",
    "        dataset_group_arn=dsg['DatasetGroupArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_group_arn is None:\n",
    "    create_dataset_group_response = forecast.create_dataset_group(DatasetGroupName=dataset_group_name, Domain=\"RETAIL\" )\n",
    "    dataset_group_arn = create_dataset_group_response['DatasetGroupArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the datasets\n",
    "\n",
    "The following cell creates the time series and related datasets, and adds them to the dataset group.\n",
    "\n",
    "The cell will wait loop and display the status until the datasets are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam = boto3.resource('iam')\n",
    "role_arn = iam.Role('ForecastRole').arn\n",
    "\n",
    "# This is the schema of the time series dataset.\n",
    "schema ={\n",
    "   \"Attributes\":[\n",
    "      {\n",
    "         \"AttributeName\":\"timestamp\",\n",
    "         \"AttributeType\":\"timestamp\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\":\"item_id\",\n",
    "         \"AttributeType\":\"string\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\":\"demand\",\n",
    "         \"AttributeType\":\"float\"\n",
    "      }\n",
    "   ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_arn = None\n",
    "dataset_list = forecast.list_datasets()\n",
    "for dataset in dataset_list['Datasets']:\n",
    "    if dataset['DatasetName']==dataset_name:\n",
    "        dataset_arn = dataset['DatasetArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_arn is None:\n",
    "    time_series_response=forecast.create_dataset(\n",
    "                        Domain=\"RETAIL\",\n",
    "                        DatasetType='TARGET_TIME_SERIES',\n",
    "                        DatasetName=dataset_name,\n",
    "                        DataFrequency=dataset_frequency, \n",
    "                        Schema = schema\n",
    "    )\n",
    "\n",
    "    dataset_arn = time_series_response['DatasetArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the import job for the time series dataset.\n",
    "dataset_import_job_name = 'EP_DSIMPORT_JOB_TARGET'\n",
    "ds_import_job_arn = None\n",
    "dataset_import_job_list = forecast.list_dataset_import_jobs()\n",
    "\n",
    "for dataset_import_job in dataset_import_job_list['DatasetImportJobs']:\n",
    "    if dataset_import_job['DatasetImportJobName'] == dataset_import_job_name:\n",
    "        ds_import_job_arn = dataset_import_job['DatasetImportJobArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the import job doesn't already exist, create it\n",
    "if ds_import_job_arn is None:\n",
    "    data_source = {\"S3Config\" : {\"Path\":s3_data_path,\"RoleArn\": role_arn} }\n",
    "    ds_import_job_response=forecast.create_dataset_import_job(DatasetImportJobName=dataset_import_job_name,\n",
    "                                                          DatasetArn=dataset_arn,\n",
    "                                                          DataSource= data_source,\n",
    "                                                          TimestampFormat=timestamp_format\n",
    "                                                         )\n",
    "    ds_import_job_arn=ds_import_job_response['DatasetImportJobArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_import_job_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# This is the schema of the related data, which contains the price.\n",
    "related_schema ={\n",
    "   \"Attributes\":[\n",
    "      {\n",
    "         \"AttributeName\":\"timestamp\",\n",
    "         \"AttributeType\":\"timestamp\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\":\"item_id\",\n",
    "         \"AttributeType\":\"string\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\":\"price\",\n",
    "         \"AttributeType\":\"float\"\n",
    "      }\n",
    "   ]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_dataset_arn = None\n",
    "dataset_list = forecast.list_datasets()\n",
    "for dataset in dataset_list['Datasets']:\n",
    "   if dataset['DatasetName']==related_dataset_name:\n",
    "        related_dataset_arn = dataset['DatasetArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if related_dataset_arn is None:\n",
    "    related_time_series_response=forecast.create_dataset(\n",
    "                        Domain=\"RETAIL\",\n",
    "                        DatasetType='RELATED_TIME_SERIES',\n",
    "                        DatasetName=related_dataset_name,\n",
    "                        DataFrequency=dataset_frequency, \n",
    "                        Schema = related_schema)\n",
    "    related_dataset_arn = related_time_series_response['DatasetArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast.describe_dataset(DatasetArn=related_dataset_arn)\n",
    "\n",
    "\n",
    "related_dataset_import_job_name = 'EP_DSIMPORT_JOB_TARGET_RELATED'\n",
    "ds_related_import_job_arn = None\n",
    "related_data_source = {\"S3Config\" : {\"Path\":s3_related_data_path,\"RoleArn\": role_arn} }\n",
    "\n",
    "dataset_import_job_list = forecast.list_dataset_import_jobs()\n",
    "\n",
    "for dataset_import_job in dataset_import_job_list['DatasetImportJobs']:\n",
    "    print(dataset_import_job)\n",
    "    if dataset_import_job['DatasetImportJobName']==related_dataset_import_job_name:\n",
    "        if dataset_import_job['Status'] == 'ACTIVE':\n",
    "            ds_related_import_job_arn = dataset_import_job['DatasetImportJobArn']\n",
    "        elif dataset_import_job['Status'] in ['CREATE_FAILED']:\n",
    "            forecast.delete_dataset_import_job(DatasetImportJobArn=dataset_import_job['DatasetImportJobArn'])\n",
    "            time.sleep(60)\n",
    "        else:\n",
    "            pass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_related_import_job_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if ds_related_import_job_arn is None:\n",
    "    ds_related_import_job_response=forecast.create_dataset_import_job(DatasetImportJobName=related_dataset_import_job_name,\n",
    "                                                          DatasetArn=related_dataset_arn,\n",
    "                                                          DataSource= related_data_source,\n",
    "                                                          TimestampFormat=timestamp_format\n",
    "                                                         )\n",
    "    ds_related_import_job_arn=ds_related_import_job_response['DatasetImportJobArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the time series and related dataset to the dataset group.\n",
    "dsg = forecast.describe_dataset_group(DatasetGroupArn=dataset_group_arn)\n",
    "print(dsg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_arn)\n",
    "print(related_dataset_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.update_dataset_group(DatasetGroupArn=dataset_group_arn, DatasetArns=[dataset_arn, related_dataset_arn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the related dataset to finish.\n",
    "status_indicator = StatusIndicator()\n",
    "\n",
    "while True:\n",
    "    status = forecast.describe_dataset_import_job(DatasetImportJobArn=ds_related_import_job_arn)['Status']\n",
    "    status_indicator.update(status)\n",
    "    if status in ('ACTIVE', 'CREATE_FAILED'): break\n",
    "    time.sleep(10)\n",
    "\n",
    "status_indicator.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the time series dataset to finish. This process typically takes longer than the related set.\n",
    "status_indicator = StatusIndicator()\n",
    "\n",
    "while True:\n",
    "    status = forecast.describe_dataset_import_job(DatasetImportJobArn=ds_import_job_arn)['Status']\n",
    "    status_indicator.update(status)\n",
    "    if status in ('ACTIVE', 'CREATE_FAILED'): break\n",
    "    time.sleep(10)\n",
    "\n",
    "status_indicator.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell stores the Amazon Resource Names (ARNs) for the forecast objects that were created previously. They can be loaded from other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store ds_import_job_arn\n",
    "%store dataset_arn\n",
    "%store dataset_group_arn\n",
    "%store related_dataset_arn\n",
    "%store ds_related_import_job_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the predictor\n",
    "\n",
    "The following cell creates the predictor by using the following parameters:\n",
    "\n",
    "- The forecast horizon is set to *30 days*.\n",
    "- *DeepAR+* is the selected algorithm. For more information, see [DeepAR+ Algorithm](https://docs.aws.amazon.com/forecast/latest/dg/aws-forecast-recipe-deeparplus.html) in the AWS Documentation.\n",
    "- Hyperparameters are specified for the algorithm. These hyperparameters were generated by running the forecast with PerformHPO set to *true*. This created a hyperparameter tuning job on the model, which produced the values that follow.\n",
    "- A single backtest window for *30 days* is used.\n",
    "- The input data configuration is set to the dataset group that was created earlier.\n",
    "- Holidays for the United Kingdom are added as supplementary features.\n",
    "- A featurization pipeline is created for the price features. For more information, see the [Handling Missing Values](https://docs.aws.amazon.com/forecast/latest/dg/howitworks-missing-values.html) topic in the AWS Documentation.\n",
    "\n",
    "The cell will wait loop and display the status until the datasets are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_name= prefix+'_deeparp_algo'\n",
    "forecast_horizon = 30\n",
    "algorithm_arn = 'arn:aws:forecast:::algorithm/Deep_AR_Plus'\n",
    "\n",
    "training_parameters =  {'context_length': '172', \n",
    "                        'epochs': '500', \n",
    "                        'learning_rate': '0.00023391131837525837', \n",
    "                        'learning_rate_decay': '0.5', \n",
    "                        'likelihood': 'student-t', \n",
    "                        'max_learning_rate_decays': '0', \n",
    "                        'num_averaged_models': '1', \n",
    "                        'num_cells': '40', \n",
    "                        'num_layers': '2', \n",
    "                        'prediction_length': '30'}\n",
    "\n",
    "evaluation_parameters= {\"NumberOfBacktestWindows\": 1, \"BackTestWindowOffset\": 30}\n",
    "\n",
    "input_data_config = {\"DatasetGroupArn\": dataset_group_arn, \"SupplementaryFeatures\": [ {\"Name\": \"holiday\",\"Value\": \"UK\"} ]}\n",
    "                  \n",
    "featurization_config= {\"ForecastFrequency\": dataset_frequency,\n",
    "                      \"Featurizations\": \n",
    "                      [\n",
    "                          {\n",
    "                            \"AttributeName\": \"price\",\n",
    "                            \"FeaturizationPipeline\": [\n",
    "                                {\n",
    "                                    \"FeaturizationMethodName\": \"filling\",\n",
    "                                    \"FeaturizationMethodParameters\": {\n",
    "                                        \"middlefill\": \"median\",\n",
    "                                        \"backfill\": \"min\",\n",
    "                                        \"futurefill\": \"max\"               \n",
    "                                        }\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                      ]}\n",
    "\n",
    "predictor_arn = None\n",
    "predictors = forecast.list_predictors()\n",
    "for predictor in predictors['Predictors']:\n",
    "    print(predictor)\n",
    "    if predictor['PredictorName'] == predictor_name:\n",
    "        predictor_arn = predictor['PredictorArn']\n",
    "\n",
    "if predictor_arn is None:\n",
    "    create_predictor_response=forecast.create_predictor(PredictorName = predictor_name, \n",
    "                                                  AlgorithmArn = algorithm_arn,\n",
    "                                                  ForecastHorizon = forecast_horizon,\n",
    "                                                  PerformAutoML = False,\n",
    "                                                  PerformHPO = False,\n",
    "                                                  EvaluationParameters= evaluation_parameters, \n",
    "                                                  InputDataConfig = input_data_config,\n",
    "                                                  FeaturizationConfig = featurization_config,\n",
    "                                                  TrainingParameters = training_parameters\n",
    "                                                 )\n",
    "    predictor_arn = create_predictor_response['PredictorArn']\n",
    "\n",
    "%store predictor_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_indicator = StatusIndicator()\n",
    "while True:\n",
    "    status = forecast.describe_predictor(PredictorArn=predictor_arn)['Status']\n",
    "    status_indicator.update(status)\n",
    "    if status in ('ACTIVE', 'CREATE_FAILED'): break\n",
    "    time.sleep(10)\n",
    "\n",
    "status_indicator.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = forecast.describe_predictor(PredictorArn=predictor_arn)\n",
    "print(f['TrainingParameters'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting accuracy metrics\n",
    "\n",
    "The next cell prints the accuracy metrics for the predictor that was just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.get_accuracy_metrics(PredictorArn=predictor_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the forecast\n",
    "\n",
    "The following cell creates a forecast from the predictor that was created earlier. \n",
    "\n",
    "The predictor and forecast ARN values are stored so that they can be retrieved from the lab notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_Name= prefix+'_deeparp_algo_forecast'\n",
    "forecast_arn = None\n",
    "forecasts = forecast.list_forecasts()\n",
    "for f in forecasts['Forecasts']:\n",
    "    if f['ForecastName']==forecast_Name:\n",
    "        forecast_arn = f['ForecastArn']\n",
    "        \n",
    "if forecast_arn is None:\n",
    "    create_forecast_response=forecast.create_forecast(ForecastName=forecast_Name,\n",
    "                                                  PredictorArn=predictor_arn)\n",
    "    forecast_arn = create_forecast_response['ForecastArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store forecast_arn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_indicator = StatusIndicator()\n",
    "while True:\n",
    "    status = forecast.describe_forecast(ForecastArn=forecast_arn)['Status']\n",
    "    status_indicator.update(status)\n",
    "    if status in ('ACTIVE', 'CREATE_FAILED'): break\n",
    "    time.sleep(10)\n",
    "\n",
    "status_indicator.end()\n",
    "\n",
    "print(forecast_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell creates a quick forecast as a test, which can be useful for troubleshooting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "forecast_response = forecast_query.query_forecast(\n",
    "    ForecastArn=forecast_arn,\n",
    "    Filters={\"item_id\":\"21232\"}\n",
    ")\n",
    "print(forecast_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional cleanup\n",
    "\n",
    "Cleanup is performed in the `forecast-lab.ipynb` notebook. If you must perform the cleanup here, change the following cell to code by selecting the cell and pressing Y. Then, run the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forecast.delete_forecast(ForecastArn=forecast_arn)\n",
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forecast.delete_predictor(PredictorArn=predictor_arn)\n",
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forecast.delete_dataset_import_job(DatasetImportJobArn=ds_import_job_arn)\n",
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forecast.delete_dataset(DatasetArn=dataset_arn)\n",
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forecast.delete_dataset_group(DatasetGroupArn=dataset_group_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
